# भारतीय भाषाओं के लिए विशाल भाषा प्रतिरूप (LLMs) - आरंभ से

## परिचय

हमने [Neural Networks: Zero To Hero](https://karpathy.ai/zero-to-hero.html) वीडियो व्याख्यान शृंखला के [इस कोड भंडार](https://github.com/karpathy/ng-video-lecture) से द्विशाख (फ़ोर्क) बना कर यह भंडार बनाया है। 

यह भंडर निम्नलिखित तरीकों से मूल भंडार से भिन्न है -
- भारतीय भाषाओं में विशाल भाषा प्रतिरूप (LLM) प्रशिक्षित करने के लिए कोड अनुकूल बनाया गया है।
- Tiny Shakespeare की जगह [AI4Bharat परियोजना़](https://ai4bharat.iitm.ac.in/) के [IndicCorp v2](https://github.com/AI4Bharat/IndicBERT/tree/main#indiccorp-v2) संग्रह का प्रयोग किया गया है। आंकड़ा समुच्चय को तैयार करने के लिए हमने `download_data_hi.sh` संचिका उपलब्ध कराई है (ध्यान रखें कि यह संचिका _हिन्दी_ आंकड़े तैयार करती है, पर अन्य भारतीय भाषाओं के लिए उपयुक्त कड़ी [यहाँ](https://github.com/AI4Bharat/IndicBERT/tree/main#indiccorp-v2) से चुनी जा सकती है)।
- इस समय हमने केवल परिवर्तक (Transformer) प्रतिरूप को संपादित किया है। बाइग्राम प्रतिरूप का कोड अंतर्विष्ट नहीं है।
- इस कोड में Hugging Face के आधार पर BPE (बाईट युग्म कुटलेखन) संकेतक (tokenizer) का प्रशिक्षण किया जाता है। प्रशिक्षण होने पर संकेतक सहेजा जाता है, और अगली बार जब भी `indic_gpt.py`चलाया जाए तब प्रशिक्षित संकेतक संचिका से ही लिया जाएगा।
- परिवर्तक प्रतिरूप भी सहेजा जाता है, ताकि अगली बार जब `indic_gpt.py` चलाया जाए तब LLM का प्रशिक्षण प्रारंभ से होने के बजाय पिछली जगह से ही जारी रखा जा सके।


## प्रयोग
पहले, सुनिश्चित करें कि आपकेे संगणन परिवेश में आवश्यक संग्रह/पैकेज अधिष्ठापित हैं। उदाहरणार्थ, यदि आप pip का प्रयोग कर रहे हैं, तो आपको शायद torch (PyTorch) और tokenizers (Hugging Face Tokenizers) ऐसे अधिष्ठापित करने पड़ेंगे—
```
pip install torch
pip install tokenizers
```

### आंकड़ा समुच्चय को तैयार करना
आंकड़ा समुच्चय को तैयार करने के लिए अगले समादेश का प्रयोग करें, जो AI4Bharat से पाठ (के पहले 500MB) डाउनलोड करता है—
```
download_data_hi.sh`
```
उपरोक्त समादेश से केवल _हिन्दी_ भाषा के लिए अंकड़े तैयार किए जाते हैं। अन्य भारतीय भाषाओं के लिए आप `download_data_hi.sh` को संपादित कर के उपयुक्त कड़ी ([यहाँ](https://github.com/AI4Bharat/IndicBERT/tree/main#indiccorp-v2) से) डाल सकते हैं।

### LLM को प्रशिक्षित करना
आंकड़ा समुच्चय तैयार होने पर आप प्रशिक्षण प्रारंभ कर सकते हैं—
```
python indic_gpt.py
```
यह चलाने पर, सबसे पहले एक BPE संकेतक प्रशिक्षित किया जाएगा और `saved_tokenizer` नाम की संचिका में सहेजा जाएगा (यदि यह संचिका पहले सहेजी गई थी, तो पुनःप्रशिक्षण नहीं होगो और इसी संचिका से प्रशिक्षित किया गया संकेतक प्राप्त किया जाएगा)।

उसके बाद, मुख्य LLM प्रशिक्षित किया जाएगा। जैसे जैसे प्रशिक्षण आगे चलता रहता है,  वैसे वैसे अब तक का सबसे अच्छा प्रतिरूप `saved_model` नाम की संचिका में सहेजा जाता रहता है।

## स्वीकृतियाँ
यह कोड मूलतः आंद्रे कार्पाथी की [Neural Networks: Zero To Hero](https://karpathy.ai/zero-to-hero.html) वीडियो व्याख्यान शृंखला से लिया गया है। अन्तर्विष्ट कोड विशेषतः [Let's build GPT: from scratch, in code, spelled out](https://www.youtube.com/watch?v=kCc8FmEb1nY) व्याख्यान के कोड पर आधारित है। उस कोड को उपलब्ध कराने के लिए मैं उसका आभारी हूँ।

## अनुज्ञापन

MIT

